{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 根據 CartPole 的遊戲，來實作 Policy Gradient\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from  RLAgent_PolicyGradient import PolicyGradient\n",
    "import numpy as np\n",
    "import keyboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建造遊戲環境\n",
    "---\n",
    "![](imgs/input_output.png \"環境參數圖\")\n",
    "[原始連結](https://github.com/openai/gym/wiki/CartPole-v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "2\n",
      "Box(4,)\n",
      "4\n",
      "[  4.80000000e+00   3.40282347e+38   4.18879020e-01   3.40282347e+38]\n",
      "[ -4.80000000e+00  -3.40282347e+38  -4.18879020e-01  -3.40282347e+38]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")                # 立竿子的遊戲\n",
    "env.seed(1)     # reproducible, general Policy gradient has high variance\n",
    "env = env.unwrapped                          # 遊戲設定，好像不要讓參數有限制\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.action_space.n)\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.shape[0])\n",
    "print(env.observation_space.high)            # 最大值\n",
    "print(env.observation_space.low)             # 最小值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 測試遊戲\n",
    "---\n",
    "Example:  \n",
    "<pre>array([ 0.10472821,  3.1417834 ,  1.96147299,  5.57941823]), 0.0, True, {}</pre> \n",
    "輸出會有四個東西：  \n",
    "1. 代表 Observation 的值  \n",
    "2. Reward (這裡注意，在還可以救起來之前，Reward 都是 1，且 IsDone 為 False) \n",
    "3. IsDone\n",
    "4. Info 好像沒有用  \n",
    "\n",
    "而** Action 是一個 Int => 0 or 1 !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 重製 & render\n",
    "# print(env.reset())\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# env.render()\n",
    "# print(env.step(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始跑結果\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 創建 Agent\n",
    "Agent = PolicyGradient(\n",
    "    env.action_space.n,\n",
    "    env.observation_space.shape[0],\n",
    "    LearningRate = 0.02,\n",
    "    RewardDecay = 0.99,\n",
    "    #IsOutputGraph = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training Part\n",
    "def TrainModel(RenderThresold = 400):\n",
    "    totalReward = 0\n",
    "    IsRender = False\n",
    "    for i in range(0, 3000):\n",
    "        # 歸零\n",
    "        observation = env.reset()\n",
    "\n",
    "        # 開始模擬\n",
    "        while True:\n",
    "            # redner 畫面\n",
    "            if(IsRender):\n",
    "                env.render()\n",
    "\n",
    "            # 選擇的動作\n",
    "            actionValue = Agent.chooseAction(observation)\n",
    "\n",
    "            # 選擇動作後 的結果\n",
    "            nextObservation, reward, IsDone, Info = env.step(actionValue)\n",
    "\n",
    "            # 存進記憶庫裡\n",
    "            Agent.storeTransition(\n",
    "                observation=observation,\n",
    "                action=actionValue,\n",
    "                reward=reward\n",
    "            )\n",
    "\n",
    "\n",
    "            if IsDone:\n",
    "                # 計算 Reward\n",
    "                if(i == 0):\n",
    "                    totalReward = np.sum(Agent.MemoryReward)\n",
    "                else:\n",
    "                    totalReward = totalReward * 0.99 + np.sum(Agent.MemoryReward) * 0.01\n",
    "                print(\"Epilson \" + format(i + 1) + \" Reward: \" + format(totalReward))\n",
    "                \n",
    "                # 判斷是否到結束\n",
    "                if(totalReward > RenderThresold):\n",
    "                    return\n",
    "\n",
    "                # 學習\n",
    "                Agent.learn()\n",
    "                break\n",
    "\n",
    "            observation = nextObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunModel():\n",
    "    observation = env.reset()\n",
    "\n",
    "    # 開始模擬\n",
    "    while True:\n",
    "        # redner 畫面\n",
    "        env.render()\n",
    "\n",
    "        # 選擇的動作\n",
    "        actionValue = Agent.chooseAction(observation)\n",
    "        \n",
    "        # 選擇動作後 的結果\n",
    "        nextObservation, reward, IsDone, Info = env.step(actionValue)\n",
    "        observation = nextObservation\n",
    "        \n",
    "        # 如果按下 Ｑ 代表結束\n",
    "        if keyboard.is_pressed(\"q\"):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epilson 1 Reward: 35.0\n",
      "Epilson 2 Reward: 34.83\n",
      "Epilson 3 Reward: 34.671699999999994\n",
      "Epilson 4 Reward: 34.784983\n",
      "Epilson 5 Reward: 34.73713316999999\n",
      "Epilson 6 Reward: 34.57976183829999\n",
      "Epilson 7 Reward: 34.403964219916986\n",
      "Epilson 8 Reward: 34.449924577717816\n",
      "Epilson 9 Reward: 34.44542533194064\n",
      "Epilson 10 Reward: 34.33097107862123\n",
      "Epilson 11 Reward: 34.37766136783502\n",
      "Epilson 12 Reward: 34.323884754156666\n",
      "Epilson 13 Reward: 34.3806459066151\n",
      "Epilson 14 Reward: 34.85683944754895\n",
      "Epilson 15 Reward: 34.698271053073455\n",
      "Epilson 16 Reward: 34.75128834254272\n",
      "Epilson 17 Reward: 34.79377545911729\n",
      "Epilson 18 Reward: 34.81583770452612\n",
      "Epilson 19 Reward: 34.93767932748086\n",
      "Epilson 20 Reward: 35.09830253420605\n",
      "Epilson 21 Reward: 35.31731950886399\n",
      "Epilson 22 Reward: 35.33414631377535\n",
      "Epilson 23 Reward: 35.1508048506376\n",
      "Epilson 24 Reward: 35.359296802131226\n",
      "Epilson 25 Reward: 35.38570383410992\n",
      "Epilson 26 Reward: 35.93184679576882\n",
      "Epilson 27 Reward: 35.81252832781114\n",
      "Epilson 28 Reward: 35.85440304453302\n",
      "Epilson 29 Reward: 35.86585901408769\n",
      "Epilson 30 Reward: 36.277200423946816\n",
      "Epilson 31 Reward: 36.45442841970735\n",
      "Epilson 32 Reward: 36.56988413551027\n",
      "Epilson 33 Reward: 36.634185294155166\n",
      "Epilson 34 Reward: 36.657843441213615\n",
      "Epilson 35 Reward: 36.64126500680148\n",
      "Epilson 36 Reward: 37.394852356733466\n",
      "Epilson 37 Reward: 37.93090383316613\n",
      "Epilson 38 Reward: 37.901594794834466\n",
      "Epilson 39 Reward: 38.26257884688612\n",
      "Epilson 40 Reward: 39.00995305841726\n",
      "Epilson 41 Reward: 39.63985352783309\n",
      "Epilson 42 Reward: 39.91345499255476\n",
      "Epilson 43 Reward: 40.434320442629215\n",
      "Epilson 44 Reward: 40.47997723820293\n",
      "Epilson 45 Reward: 41.2151774658209\n",
      "Epilson 46 Reward: 41.493025691162686\n",
      "Epilson 47 Reward: 42.35809543425106\n",
      "Epilson 48 Reward: 43.45451447990855\n",
      "Epilson 49 Reward: 43.75996933510947\n",
      "Epilson 50 Reward: 44.24236964175837\n",
      "Epilson 51 Reward: 44.86994594534079\n",
      "Epilson 52 Reward: 44.91124648588738\n",
      "Epilson 53 Reward: 45.15213402102851\n",
      "Epilson 54 Reward: 45.590612680818225\n",
      "Epilson 55 Reward: 45.87470655401005\n",
      "Epilson 56 Reward: 45.82595948846994\n",
      "Epilson 57 Reward: 45.55769989358524\n",
      "Epilson 58 Reward: 46.632122894649385\n",
      "Epilson 59 Reward: 47.265801665702895\n",
      "Epilson 60 Reward: 47.943143649045865\n",
      "Epilson 61 Reward: 48.63371221255541\n",
      "Epilson 62 Reward: 49.037375090429855\n",
      "Epilson 63 Reward: 49.54700133952556\n",
      "Epilson 64 Reward: 50.2115313261303\n",
      "Epilson 65 Reward: 51.34941601286899\n",
      "Epilson 66 Reward: 52.99592185274031\n",
      "Epilson 67 Reward: 55.31596263421291\n",
      "Epilson 68 Reward: 56.01280300787078\n",
      "Epilson 69 Reward: 57.44267497779207\n",
      "Epilson 70 Reward: 58.748248228014155\n",
      "Epilson 71 Reward: 59.20076574573401\n",
      "Epilson 72 Reward: 60.138758088276674\n",
      "Epilson 73 Reward: 60.75737050739391\n",
      "Epilson 74 Reward: 61.45979680231997\n",
      "Epilson 75 Reward: 61.42519883429677\n",
      "Epilson 76 Reward: 62.5009468459538\n",
      "Epilson 77 Reward: 62.965937377494264\n",
      "Epilson 78 Reward: 63.01627800371932\n",
      "Epilson 79 Reward: 63.36611522368212\n",
      "Epilson 80 Reward: 64.0024540714453\n",
      "Epilson 81 Reward: 64.48242953073084\n",
      "Epilson 82 Reward: 65.49760523542353\n",
      "Epilson 83 Reward: 65.9726291830693\n",
      "Epilson 84 Reward: 66.87290289123861\n",
      "Epilson 85 Reward: 67.41417386232622\n",
      "Epilson 86 Reward: 69.15003212370296\n",
      "Epilson 87 Reward: 72.70853180246593\n",
      "Epilson 88 Reward: 75.33144648444126\n",
      "Epilson 89 Reward: 79.11813201959686\n",
      "Epilson 90 Reward: 80.47695069940089\n",
      "Epilson 91 Reward: 86.88218119240688\n",
      "Epilson 92 Reward: 96.63335938048282\n",
      "Epilson 93 Reward: 102.12702578667798\n",
      "Epilson 94 Reward: 103.2157555288112\n",
      "Epilson 95 Reward: 106.1635979735231\n",
      "Epilson 96 Reward: 112.50196199378787\n",
      "Epilson 97 Reward: 116.21694237385\n",
      "Epilson 98 Reward: 119.5747729501115\n",
      "Epilson 99 Reward: 129.72902522061037\n",
      "Epilson 100 Reward: 129.47173496840426\n",
      "Epilson 101 Reward: 130.5670176187202\n",
      "Epilson 102 Reward: 130.741347442533\n",
      "Epilson 103 Reward: 131.7339339681077\n",
      "Epilson 104 Reward: 132.84659462842663\n",
      "Epilson 105 Reward: 133.95812868214236\n",
      "Epilson 106 Reward: 135.63854739532096\n",
      "Epilson 107 Reward: 137.18216192136774\n",
      "Epilson 108 Reward: 137.50034030215406\n",
      "Epilson 109 Reward: 137.96533689913252\n",
      "Epilson 110 Reward: 137.82568353014122\n",
      "Epilson 111 Reward: 140.8674266948398\n",
      "Epilson 112 Reward: 141.09875242789138\n",
      "Epilson 113 Reward: 140.69776490361247\n",
      "Epilson 114 Reward: 145.90078725457636\n",
      "Epilson 115 Reward: 148.3917793820306\n",
      "Epilson 116 Reward: 148.10786158821028\n",
      "Epilson 117 Reward: 159.1867829723282\n",
      "Epilson 118 Reward: 161.92491514260493\n",
      "Epilson 119 Reward: 162.81566599117886\n",
      "Epilson 120 Reward: 164.7375093312671\n",
      "Epilson 121 Reward: 167.20013423795444\n",
      "Epilson 122 Reward: 167.18813289557488\n",
      "Epilson 123 Reward: 167.20625156661913\n",
      "Epilson 124 Reward: 170.24418905095294\n",
      "Epilson 125 Reward: 171.4817471604434\n",
      "Epilson 126 Reward: 171.29692968883896\n",
      "Epilson 127 Reward: 173.62396039195056\n",
      "Epilson 128 Reward: 174.45772078803105\n",
      "Epilson 129 Reward: 174.28314358015072\n",
      "Epilson 130 Reward: 175.6603121443492\n",
      "Epilson 131 Reward: 175.2737090229057\n",
      "Epilson 132 Reward: 177.30097193267665\n",
      "Epilson 133 Reward: 178.27796221334987\n",
      "Epilson 134 Reward: 179.33518259121638\n",
      "Epilson 135 Reward: 180.4818307653042\n",
      "Epilson 136 Reward: 182.12701245765115\n",
      "Epilson 137 Reward: 182.08574233307465\n",
      "Epilson 138 Reward: 185.1948849097439\n",
      "Epilson 139 Reward: 194.79293606064644\n",
      "Epilson 140 Reward: 195.73500670003995\n",
      "Epilson 141 Reward: 197.92765663303956\n",
      "Epilson 142 Reward: 198.8083800667092\n",
      "Epilson 143 Reward: 221.0702962660421\n",
      "Epilson 144 Reward: 221.57959330338167\n",
      "Epilson 145 Reward: 229.66379737034785\n",
      "Epilson 146 Reward: 253.0771593966444\n",
      "Epilson 147 Reward: 257.58638780267796\n",
      "Epilson 148 Reward: 281.7705239246512\n",
      "Epilson 149 Reward: 337.1128186854047\n",
      "Epilson 150 Reward: 367.00169049855066\n",
      "Epilson 151 Reward: 555.4916735935651\n"
     ]
    }
   ],
   "source": [
    "# 訓練 Model\n",
    "TrainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使否要顯示 Model\n",
    "# 按 Q 結束\n",
    "RunModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 關閉程式\n",
    "Agent.session.close()\n",
    "env.render(close=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
